{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pmair\\AppData\\Local\\Temp\\ipykernel_4544\\945606445.py:6: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI  # require v1.33\n",
    "import shelve\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPEN_API_KEY_Bsc\")\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getenv(\"OPEN_API_KEY_Bsc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code might need optimization - very simple solution but works for now\n",
    "\n",
    "\"\"\"\n",
    "creates a new thread file to store conversations, or loads old conversations if they exist\n",
    "\"\"\"\n",
    "\n",
    "# initialize thread dict\n",
    "thread_dict = {\n",
    "    \"prompt\": [],\n",
    "    \"answer\": [],\n",
    "    \"user_id\": [],\n",
    "    \"thread_id\": [],\n",
    "    \"msg_id\": [],\n",
    "    \"assistant_id\": []  \n",
    "}\n",
    "\n",
    "# check if already threads exist\n",
    "if os.path.exists(\"GPT_Threads\\\\threads.json\"):\n",
    "    # load exisiting thread\n",
    "    with open('GPT_Threads\\\\threads.json') as json_file:\n",
    "        data = json.load(json_file)  # this is a list of dictionaries\n",
    "        for elem in data: \n",
    "            thread_dict[\"thread_id\"].append(elem[\"thread_id\"])\n",
    "            thread_dict[\"msg_id\"].append(elem[\"msg_id\"])\n",
    "            thread_dict[\"assistant_id\"].append(elem[\"assistant_id\"])\n",
    "            thread_dict[\"prompt\"].append(elem[\"prompt\"])\n",
    "            thread_dict[\"answer\"].append(elem[\"answer\"])\n",
    "            thread_dict[\"user_id\"].append(elem[\"user_id\"])\n",
    "elif not os.path.exists(\"GPT_Threads\"): \n",
    "    os.makedirs(\"GPT_Threads\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_code_interpreter_files(path):\n",
    "    \"\"\"\n",
    "    uploads file for the assistant to use as external knowledge to the openai server\n",
    "    \"\"\"\n",
    "    file_ids = []\n",
    "    # Upload files from Dashboard Files with an \"assistants\" purpose\n",
    "    for file_name in path:\n",
    "        with open(file_name, \"rb\") as file_data:\n",
    "            file_response = client.files.create(file=file_data, purpose='assistants')\n",
    "            file_ids.append(file_response.id) \n",
    "    return file_ids\n",
    "\n",
    "def upload_file_search_files(file_paths, vector_store):\n",
    "    \"\"\"\n",
    "    uploads file for the assistant to use as external knowledge to the openai server\n",
    "    \"\"\"\n",
    "    file_streams = [open(path, \"rb\") for path in file_paths]\n",
    " \n",
    "    # Use the upload and poll SDK helper to upload the files, add them to the vector store,\n",
    "    # and poll the status of the file batch for completion.\n",
    "    file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "    vector_store_id=vector_store.id, files=file_streams)\n",
    "    print(file_batch.status)\n",
    "    print(file_batch.file_counts)\n",
    "    return \n",
    "\n",
    "def create_assistant(file_ids, vector_ids):\n",
    "    \"\"\"\n",
    "    creates the assistant with given instruction and files for external knowledge\n",
    "    choose which model you want to use, note that it has to have retrieval and code_interpreter functionalities\n",
    "    more information on the openai assistant website\n",
    "    \"\"\"\n",
    "    assistant = client.beta.assistants.create(\n",
    "        name=\"Dashboard Onboarding ChatBot BetaV2\",\n",
    "        temperature=1,\n",
    "        instructions=\"You are an assistant during a dashboard onboarding process. You should guide users of different expertise level step by step through a comprehensive journey of getting to know a PowerBI dashboard. The elements and layout of the dashboard are explained in the provided JSON file. I also provided you with previous user interactions in the form of csv files, which you should use to guide users to insightful states of the dashboard. Start the onboarding by asking the experience level of the new user and then give exercises based on this information. Beginner users should at least solve 5 exercises, showing them how to leverage PowerBI for data analysis, but also introduce them to the given dashboard. Only prompt one step at a time to the user, do not overwhelm him with too much information at once. Only give exercise you have knowledge about, hence you should be able to identify if the users answer is incorrect. Make sure to leverage the uploaded files\",\n",
    "        model=\"gpt-4o-2024-05-13\",\n",
    "        tools=[{\"type\": \"file_search\"}, {\"type\": \"code_interpreter\"}],\n",
    "        tool_resources = {\n",
    "            \"file_search\": {\"vector_store_ids\": [vector_ids]}\n",
    "            ,\"code_interpreter\": {\"file_ids\": file_ids}\n",
    "        }\n",
    "        \n",
    "    )\n",
    "    return assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thread management\n",
    "\n",
    "def thread_management(user_id, name):\n",
    "\n",
    "    if user_id == \"\" or name == \"\":\n",
    "        return None, None, \"Invalid User ID or name\"\n",
    "    thread_id = check_if_thread_exists(user_id)\n",
    "\n",
    "    # If a thread doesn't exist, create one and store it\n",
    "    if thread_id is None:\n",
    "        msg=f\"Created new thread for user_id {user_id} - Welcome {name}!\"\n",
    "        thread = client.beta.threads.create()\n",
    "        store_thread(user_id, thread.id)\n",
    "        thread_id = thread.id\n",
    "\n",
    "    # Otherwise, retrieve the existing thread\n",
    "    else:\n",
    "        msg=f\"Found existing thread for user ID {user_id} - Welcome Back {name}!\"\n",
    "        thread = client.beta.threads.retrieve(thread_id)\n",
    "    return thread_id, thread, msg\n",
    "\n",
    "def check_if_thread_exists(user_id):\n",
    "    with shelve.open(\"threads_db\") as threads_shelf:\n",
    "        return threads_shelf.get(user_id, None)\n",
    "\n",
    "def store_thread(user_id, thread_id):\n",
    "    with shelve.open(\"threads_db\", writeback=True) as threads_shelf:\n",
    "        threads_shelf[user_id] = thread_id\n",
    "\n",
    "def save_thread(user_id, data_user, data_assistant):\n",
    "    # saving threads in json file for better visualization of conversations\n",
    "    thread_dict[\"thread_id\"].append(data_assistant.thread_id)\n",
    "    thread_dict[\"msg_id\"].append(data_assistant.id)\n",
    "    thread_dict[\"assistant_id\"].append(data_assistant.assistant_id)\n",
    "    thread_dict[\"prompt\"].append(data_user.content[0].text.value)\n",
    "    thread_dict[\"answer\"].append(data_assistant.content[0].text.value)\n",
    "    thread_dict[\"user_id\"].append(user_id)\n",
    "\n",
    "    df = pd.DataFrame(thread_dict)  # probably also a more neat solution without this step available\n",
    "    df.to_json(\"GPT_Threads\\\\threads.json\", orient=\"records\", indent=2)\n",
    "\n",
    "# Generate response\n",
    "def generate_response(user_id, name, message_body, temp):\n",
    "    \n",
    "    thread_id, thread, _ = thread_management(user_id, name)\n",
    "    if thread_id is None: \n",
    "        return \"invalid request\"\n",
    "\n",
    "    # Add message to thread\n",
    "    message = client.beta.threads.messages.create(\n",
    "        thread_id=thread_id,\n",
    "        role=\"user\",\n",
    "        content=message_body,\n",
    "    )\n",
    "\n",
    "    # Run the assistant and get the new message\n",
    "    new_message = run_assistant(user_id, thread, temp)\n",
    "    # print(f\"To {name}:\", new_message)\n",
    "    return new_message\n",
    "\n",
    "# Run assistant\n",
    "def run_assistant(user_id, thread, temp):\n",
    "    # Retrieve the created assistant or paste the id of any other available assistant instead of \"assistant_glob.id\"\n",
    "\n",
    "    # Onboarding bot: asst_CzAZx0pbuCdy57fd18DFWZEB\n",
    "    # Dashboard Onboarding Assistant: asst_irK6D1q8nwG8JTHN21ykPURB\n",
    "    # Dashboard Onboarding ChatBot BetaV2: asst_FaRkIb4JWeNMBij48tjyGyS2\n",
    "\n",
    "    assistant = client.beta.assistants.retrieve(\"asst_FaRkIb4JWeNMBij48tjyGyS2\")\n",
    "    assistant = client.beta.assistants.update(\n",
    "        assistant_id=assistant.id,\n",
    "        temperature=temp\n",
    ")\n",
    "\n",
    "    # Run the assistant\n",
    "    run = client.beta.threads.runs.create_and_poll(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant.id,\n",
    "    )\n",
    "\n",
    "    # Wait for completion\n",
    "    # while run.status != \"completed\":\n",
    "    #     time.sleep(0.5)\n",
    "    #     run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n",
    "\n",
    "    # Retrieve the Messages\n",
    "    messages = client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id)\n",
    "    new_message = messages.data[0].content[0].text\n",
    "\n",
    "    annotations = new_message.annotations\n",
    "    citations = []\n",
    "    for index, annotation in enumerate(annotations):\n",
    "        new_message.value = new_message.value.replace(annotation.text, f\"[{index}]\")\n",
    "        if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "            cited_file = client.files.retrieve(file_citation.file_id)\n",
    "            citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "    print(new_message.value)\n",
    "    print(\"\\n\".join(citations))\n",
    "\n",
    "    data_assistant = client.beta.threads.messages.list(thread_id=thread.id).data[0]\n",
    "    data_user = client.beta.threads.messages.list(thread_id=thread.id).data[1]\n",
    "\n",
    "    save_thread(user_id, data_user, data_assistant)\n",
    "    return new_message.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "FileCounts(cancelled=0, completed=1, failed=0, in_progress=0, total=1)\n"
     ]
    }
   ],
   "source": [
    "# Create an assistant with external knowledge\n",
    "# only run once - check on website if assitant was created\n",
    "\n",
    "# Upload files for code interpreter\n",
    "files = upload_code_interpreter_files([\"Dashboard Files\\merged_file.csv\"])\n",
    "\n",
    "# create vector store\n",
    "vector_store = client.beta.vector_stores.create(name=\"Component Graph\")\n",
    "upload_file_search_files([\"Dashboard Files\\Component Graph.md\"], vecor_store)\n",
    "\n",
    "# Create assistant\n",
    "assistant_glob = create_assistant(files, vector_store.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 4.19.2, however version 4.29.0 is available, please upgrade.\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "theme = gr.themes.Monochrome(radius_size=\"md\", spacing_size=\"lg\")\n",
    "\n",
    "with gr.Blocks(theme=theme) as demo:\n",
    "\n",
    "    gr.Markdown(\"# Welcome to PowerBI Onboarding!\")\n",
    "    greet = gr.Markdown(\"Please enter your User Name and ID\")\n",
    "    user_info = gr.Markdown(visible=False)\n",
    "\n",
    "    with gr.Column(visible=True) as user_int:\n",
    "        userid = gr.Textbox(label=\"User ID\")\n",
    "        name = gr.Textbox(label=\"User Name\")\n",
    "        button_submit = gr.Button(\"Submit\")\n",
    "\n",
    "    thread_msg = gr.Markdown(label=\"Thread\")\n",
    "    message = gr.Textbox(label=\"Message\", visible=False)\n",
    "\n",
    "    # Define the function and its inputs and outputs\n",
    "    with gr.Row(visible=False) as btn_int:\n",
    "        button_send = gr.Button(\"Send\")\n",
    "        button_clear = gr.ClearButton([message])\n",
    "        temp = gr.Slider(minimum=0, maximum=2, step=0.1, value=1, interactive=True, label=\"Temperature\")  # for testing purpose only\n",
    "    bot = gr.Textbox(label=\"Chat Bot\", visible=False)\n",
    "    \n",
    "    def submit(userid, name):\n",
    "        if len(name) == 0:    \n",
    "            return \"Empty name or ID\"\n",
    "        time.sleep(2)\n",
    "        return {btn_int: gr.Row(visible=True),\n",
    "                message: gr.Textbox(visible=True),\n",
    "                bot: gr.Textbox(visible=True),\n",
    "                user_int: gr.Column(visible=False),\n",
    "                temp: gr.Slider(visible=True),\n",
    "            \n",
    "                greet: gr.Markdown(visible=False),\n",
    "                user_info: gr.Markdown(f\"{name} with User ID: {userid}\", visible=True)\n",
    "                }\n",
    "\n",
    "    button_submit.click(fn=thread_management, inputs=[userid, name], outputs=[gr.Text(visible=False), gr.Text(visible=False), thread_msg])\n",
    "    button_submit.click(fn=submit, inputs=[userid, name], outputs=[btn_int, message, bot, user_int, greet, user_info, temp])\n",
    "    button_send.click(fn=generate_response, inputs=[userid, name, message, temp], outputs=bot)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = gr.Interface(\n",
    "    fn=generate_response,\n",
    "    inputs=[\"text\", \"text\", \"text\"],\n",
    "    outputs=[\"text\"],\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test assistant\n",
    "user_name = \"Beginner\"\n",
    "user_id = \"2\"\n",
    "new_message = generate_response(\"Start Dashboard onboarding\", user_id, user_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_message = generate_response(\"Beginner\", user_id, user_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_message = generate_response(\"I see two summaries of new hire numbers, a comparison chart of two years. A line chart showing Full Time vs Part Time hires and 2 filter options. Did i miss something?\", user_id, user_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_message = generate_response(\"I would expect the total number of new hires to go down, since we are narrowing the data. Also maybe new trends emerge if we set filters to specific regions\", user_id, user_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_message = generate_response(\"Selecting a specific region bar will apply the same filter as if I filtered by this region using the slicer I suppose\", user_id, user_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_message = generate_response(\"Which insights should I search for in this particular dashboard?\", user_id, user_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
